{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d5ff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TaPR_pkg import etapr\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "import sklearn.preprocessing as pp\n",
    "from scipy.special import softmax\n",
    "import os\n",
    "data_dir = './data/'\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "model_dir = './models/'\n",
    "\n",
    "os.makedirs('./submissions', exist_ok=True)\n",
    "submit_dir = './submissions/'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import * \n",
    "from tensorflow_addons.optimizers import Lookahead, AdamW\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.nn import gelu\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac1e904",
   "metadata": {},
   "source": [
    "# Fit Scaler First with training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "210c5991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1004402, 86)\n"
     ]
    }
   ],
   "source": [
    "train1 = pd.read_csv(data_dir + 'train/train1.csv').drop(columns='timestamp')\n",
    "train2 = pd.read_csv(data_dir + 'train/train2.csv').drop(columns='timestamp')\n",
    "train3 = pd.read_csv(data_dir + 'train/train3.csv').drop(columns='timestamp')\n",
    "train4 = pd.read_csv(data_dir + 'train/train4.csv').drop(columns='timestamp')\n",
    "train5 = pd.read_csv(data_dir + 'train/train5.csv').drop(columns='timestamp')\n",
    "train6 = pd.read_csv(data_dir + 'train/train6.csv').drop(columns='timestamp')\n",
    "temp = pd.concat((train1, train2, train3, train4, train5, train6), axis=0)\n",
    "scaler = pp.MinMaxScaler() \n",
    "print(temp.shape)\n",
    "scaler.fit(temp)\n",
    "del temp\n",
    "\n",
    "submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "validation = pd.read_csv(data_dir + 'validation/validation.csv').drop(columns='timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1f3c61",
   "metadata": {},
   "source": [
    "# Placeholder of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d867ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_df = pd.DataFrame(pd.read_csv(data_dir + 'validation/validation.csv').iloc[:, -1])\n",
    "test_pred_df = pd.DataFrame(data=None, index=submission.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c1bfc",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44c622f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_timeseries_1d(x_array, y_array=None, timestep=1, predstep=1, skipstep=1, verbose=0):\n",
    "    x_time = []\n",
    "    if y_array is not None:\n",
    "        y_time = []\n",
    "    i = 0\n",
    "    while (i+timestep+predstep) <= x_array.shape[0]:\n",
    "        X = x_array[i : i+timestep]\n",
    "        x_time.append(X)\n",
    "        if y_array is not None:\n",
    "            Y = y_array[i+timestep : i+timestep+predstep]\n",
    "            y_time.append(Y)\n",
    "            \n",
    "        i += skipstep\n",
    "\n",
    "        if verbose != 0:\n",
    "            if i % verbose == 0:\n",
    "                print(i)\n",
    "        \n",
    "    if y_array is not None:\n",
    "        return np.array(x_time), np.array(y_time)\n",
    "    else:\n",
    "        return np.array(x_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e09e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_gradient(df):\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    feature_names = list(df)\n",
    "    grad_feature_names = []\n",
    "    \n",
    "    grads = np.gradient(df, axis=0)\n",
    "\n",
    "    for col in feature_names:\n",
    "        grad_feature_names.append(col+'_grad')\n",
    "    \n",
    "    new_df[grad_feature_names] = grads\n",
    "    return new_df.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe0eb687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(error, threshold, show_val_att=False, num_data=5000):\n",
    "    i = 0\n",
    "    while i < error.shape[0]:\n",
    "        figure, (ax1) = plt.subplots(nrows=1, ncols=1)\n",
    "        figure.set_size_inches(18, 5)\n",
    "        ax1.plot(np.array(error)[i:i+num_data])\n",
    "        ax1.plot(np.array([threshold]*num_data))\n",
    "        if show_val_att:\n",
    "            ax1.plot(np.array(val_pred_df['attack'])[i:i+num_data])\n",
    "        plt.ylim(0, threshold*5)\n",
    "        plt.show()\n",
    "        i += num_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a244f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(true, pred, verbose=True):\n",
    "    TaPR = etapr.evaluate_haicon(anomalies=true, \n",
    "                          predictions=pred\n",
    "                         )\n",
    "    if verbose:\n",
    "        print(f\"F1: {TaPR['f1']:.3f} (TaP: {TaPR['TaP']:.3f}, TaR: {TaPR['TaR']:.3f})\")\n",
    "        print(f\"Detected anomalies: {len(TaPR['Detected_Anomalies'])}\")\n",
    "    return np.float64(TaPR['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0ab9e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ma_smoothing(array, ma=5):\n",
    "    ma_ar = []\n",
    "    if ma <= 1:\n",
    "        return array\n",
    "    for i in range(ma):\n",
    "        ma_ar.append(\n",
    "            np.concatenate((np.zeros(i), np.roll(array, i)[i:])\n",
    "                          ))    \n",
    "    ma_ar = np.stack(ma_ar, axis=1).mean(axis=1)\n",
    "    return np.concatenate((np.zeros(ma-1), ma_ar[ma-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "391716ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anomaly_score(true, pred, dynamic=0):\n",
    "    e = np.abs(true-pred).mean(axis=-1)\n",
    "    if dynamic == 0:\n",
    "        return e\n",
    "    else:\n",
    "        e = ma_smoothing(e, dynamic)\n",
    "        return np.array(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59a232a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(anomaly_score, threshold, padding=False):\n",
    "    preds = []\n",
    "    for val in anomaly_score:\n",
    "        if val >= threshold:\n",
    "            preds.append(1)\n",
    "        else:\n",
    "            preds.append(0)       \n",
    "    if padding:\n",
    "        preds = list(np.zeros(TIMESTEP).astype(int)) + preds\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "825bea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFRecord Functions\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    " \n",
    "def serialize_array(array):\n",
    "    array = tf.io.serialize_tensor(array)\n",
    "    return array\n",
    "\n",
    "def single_example(x, y):\n",
    "    feature = {\n",
    "        'timestep' : _int64_feature(x.shape[0]),\n",
    "        'features' : _int64_feature(x.shape[1]),\n",
    "        'x_array' : _bytes_feature(serialize_array(x)),\n",
    "        'y_array' : _bytes_feature(serialize_array(y)) #numpy array : tf.io.serialize_tensor(array)\n",
    "            }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
    "\n",
    "def parse_and_decode(record):\n",
    "    name_to_features = {'timestep' : tf.io.FixedLenFeature([], tf.int64), \n",
    "                       'features' : tf.io.FixedLenFeature([], tf.int64), \n",
    "                       'x_array' : tf.io.FixedLenFeature([], tf.string), \n",
    "                       'y_array' : tf.io.FixedLenFeature([], tf.string)\n",
    "                       }\n",
    "    parsed = tf.io.parse_single_example(record, name_to_features)\n",
    "    x = tf.io.parse_tensor(parsed['x_array'], out_type='float32')\n",
    "    y = tf.io.parse_tensor(parsed['y_array'], out_type='float32')\n",
    "    x = tf.reshape(x, (parsed['timestep'], parsed['features']))\n",
    "    y = tf.reshape(y, (1, 86))\n",
    "    return (x, y)\n",
    "\n",
    "\n",
    "def write_train_tfrecs(suffix, test_size=0.1, shuffle=False):\n",
    "    os.makedirs('./tfrecs', exist_ok=True)\n",
    "\n",
    "    for i, df in enumerate([train1, train2, train3, train4, train5, train6]):\n",
    "        df = pd.DataFrame(scaler.transform(df), index=df.index, columns=df.columns).ewm(alpha=0.9).mean().astype('float32')\n",
    "        df = df_gradient(df)\n",
    "\n",
    "        x_data, y_data = to_timeseries_1d(df, df.iloc[:, :86], timestep=TIMESTEP, predstep=1, skipstep=1)\n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size=test_size, shuffle=shuffle, random_state=18)\n",
    "            \n",
    "        print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape)\n",
    "\n",
    "        train_filename = f\"./tfrecs/train{TIMESTEP}-{i}-{suffix}.tfrecs\"\n",
    "        valid_filename = f\"./tfrecs/valid{TIMESTEP}-{i}-{suffix}.tfrecs\"\n",
    "\n",
    "        with tf.io.TFRecordWriter(train_filename) as train_writer:\n",
    "            for j in tqdm(range(x_train.shape[0])):#zip(x_train, y_train)):\n",
    "                xtr = x_train[j]\n",
    "                ytr = y_train[j]\n",
    "                train_example = single_example(xtr, ytr)\n",
    "                train_writer.write(train_example)\n",
    "\n",
    "        del x_train, y_train, train_writer\n",
    "\n",
    "        with tf.io.TFRecordWriter(valid_filename) as valid_writer:\n",
    "            for j in tqdm(range(x_valid.shape[0])):#zip(x_train, y_train)):\n",
    "                xva = x_valid[j]\n",
    "                yva = y_valid[j]\n",
    "                valid_example = single_example(xva, yva)\n",
    "                valid_writer.write(valid_example)\n",
    "        del x_valid, y_valid, valid_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a66b044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_dataset():\n",
    "    validation = pd.read_csv(data_dir + 'validation/validation.csv').drop(columns='timestamp')\n",
    "    VALID_ATTACK = validation['attack']\n",
    "    validation = validation.drop(columns='attack')\n",
    "    validation = pd.DataFrame(scaler.transform(validation), \n",
    "                              index=validation.index, columns=validation.columns).ewm(alpha=0.9).mean().astype('float32')\n",
    "    \n",
    "    validation = df_gradient(validation)\n",
    "\n",
    "    X, Y = to_timeseries_1d(validation, validation.iloc[:, :86], timestep=TIMESTEP)\n",
    "    _, attack = to_timeseries_1d(VALID_ATTACK, VALID_ATTACK, timestep=TIMESTEP)\n",
    "    attack = np.squeeze(attack)\n",
    "    \n",
    "    return X, Y, attack\n",
    "\n",
    "\n",
    "def get_test_dataset():    \n",
    "    test1 = pd.read_csv(data_dir + 'test/test1.csv').drop(columns='timestamp')\n",
    "    test2 = pd.read_csv(data_dir + 'test/test2.csv').drop(columns='timestamp')\n",
    "    test3 = pd.read_csv(data_dir + 'test/test3.csv').drop(columns='timestamp')\n",
    "\n",
    "    test1 = pd.DataFrame(scaler.transform(test1), index=test1.index, columns=test1.columns).ewm(alpha=0.9).mean().astype('float32')\n",
    "    test1 = df_gradient(test1)\n",
    "    \n",
    "    test2 = pd.DataFrame(scaler.transform(test2), index=test2.index, columns=test2.columns).ewm(alpha=0.9).mean().astype('float32')\n",
    "    test2 = df_gradient(test2)\n",
    "    \n",
    "    test3 = pd.DataFrame(scaler.transform(test3), index=test3.index, columns=test3.columns).ewm(alpha=0.9).mean().astype('float32')\n",
    "    test3 = df_gradient(test3)\n",
    "    \n",
    "    Xt1, Yt1 = to_timeseries_1d(test1, test1.iloc[:, :86], timestep=TIMESTEP)\n",
    "    Xt2, Yt2 = to_timeseries_1d(test2, test2.iloc[:, :86], timestep=TIMESTEP)\n",
    "    Xt3, Yt3 = to_timeseries_1d(test3, test3.iloc[:, :86], timestep=TIMESTEP)\n",
    "    \n",
    "    return (Xt1, Xt2, Xt3), (Yt1, Yt2, Yt3)\n",
    "\n",
    "def get_train_dataset(suffix, test_size=0.1, shuffle=True, write=True):\n",
    "    assert 0 < test_size < 1\n",
    "    \n",
    "    if write:\n",
    "        write_train_tfrecs(suffix, test_size, shuffle)\n",
    "    \n",
    "    train_data = glob(f\"./tfrecs/train{TIMESTEP}-*-{suffix}.tfrecs\")\n",
    "    valid_data = glob(f\"./tfrecs/valid{TIMESTEP}-*-{suffix}.tfrecs\")\n",
    "    \n",
    "    print(train_data)\n",
    "    print(valid_data)\n",
    "    \n",
    "    train_dataset = tf.data.TFRecordDataset(train_data, num_parallel_reads=AUTO)\n",
    "    train_dataset = train_dataset.map(parse_and_decode, num_parallel_calls=AUTO)\n",
    "    train_dataset = train_dataset.shuffle(4096)                  \n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE, num_parallel_calls=AUTO)\n",
    "    train_dataset = train_dataset.prefetch(buffer_size=AUTO)\n",
    "    \n",
    "    valid_dataset = tf.data.TFRecordDataset(valid_data)\n",
    "    valid_dataset = valid_dataset.map(parse_and_decode, num_parallel_calls=AUTO)\n",
    "    valid_dataset = valid_dataset.batch(BATCH_SIZE*2, num_parallel_calls=AUTO)\n",
    "    valid_dataset = valid_dataset.cache()\n",
    "    valid_dataset = valid_dataset.prefetch(buffer_size=AUTO)\n",
    "    \n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57a1220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling Functions\n",
    "def mixer_block(w, token_mixing, channel_mixing, dropout=0.5, activation=tf.nn.gelu):\n",
    "    x = w\n",
    "    num_patches = x.shape[1]\n",
    "    embedding_dim = x.shape[-1]\n",
    "    \n",
    "    mlp1 = tf.keras.Sequential([\n",
    "        Dense(token_mixing, activation=activation), \n",
    "        Dropout(dropout),\n",
    "        Dense(num_patches), \n",
    "        \n",
    "    ])\n",
    "    mlp2 = tf.keras.Sequential([\n",
    "        Dense(channel_mixing, activation=activation), \n",
    "        Dropout(dropout),\n",
    "        Dense(embedding_dim)\n",
    "    ])\n",
    "    x1 = LayerNormalization()(x)\n",
    "    x1 = Permute((2, 1))(x1)\n",
    "    x1 = mlp1(x1)\n",
    "    x1 = Permute((2, 1))(x1)\n",
    "    x_res = Add()([x1, x])\n",
    "    x2 = LayerNormalization()(x_res)\n",
    "    x2 = mlp2(x2)\n",
    "    x = Add()([x_res, x2])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84284d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mixer(opt=None, num_blocks=3, token_mixing=64, channel_mixing=256, embed=128, dropout=0):\n",
    "    activation = tf.nn.gelu\n",
    "\n",
    "    \n",
    "    model_in = Input(shape=(TIMESTEP, 86*2))\n",
    "    x = Dense(embed)(model_in)\n",
    "    \n",
    "    for _ in range(num_blocks):\n",
    "        x = mixer_block(x, token_mixing, channel_mixing, dropout=dropout)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = tf.expand_dims(x, axis=1)\n",
    "    model_out = Dense(86, activation='linear', name='Output', kernel_initializer='glorot_normal', dtype='float32')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(model_in, model_out)\n",
    "    if opt is not None:\n",
    "        model.compile(loss='mae', optimizer=opt, metrics='mae')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb4ee494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMModel(tf.keras.Model):\n",
    "    #https://github.com/sayakpaul/Sharpness-Aware-Minimization-TensorFlow\n",
    "\n",
    "    def __init__(self, build_fn, rho=0.05, params={}):\n",
    "        \"\"\"\n",
    "        p, q = 2 for optimal results as suggested in the paper\n",
    "        (Section 2)\n",
    "        \"\"\"\n",
    "        super(SAMModel, self).__init__()\n",
    "        self.model = build_fn(**params)\n",
    "        self.rho = rho\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        return self.model(inputs, training=training)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        (x, y) = data\n",
    "        e_ws = []\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(x)\n",
    "            loss = self.compiled_loss(y, predictions)\n",
    "        trainable_params = self.model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_params)\n",
    "        grad_norm = self._grad_norm(gradients)\n",
    "        scale = self.rho / (grad_norm + 1e-12)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(x)\n",
    "            loss = self.compiled_loss(y, predictions)    \n",
    "        for (grad, param) in zip(gradients, trainable_params):\n",
    "            e_w = grad * scale\n",
    "            param.assign_add(e_w)\n",
    "            e_ws.append(e_w)\n",
    "        sam_gradients = tape.gradient(loss, trainable_params)\n",
    "        for (param, e_w) in zip(trainable_params, e_ws):\n",
    "            param.assign_sub(e_w)\n",
    "        \n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(sam_gradients, trainable_params))\n",
    "\n",
    "        self.compiled_metrics.update_state(y, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        (x, y) = data\n",
    "        predictions = self.model(x, training=False)\n",
    "        loss = self.compiled_loss(y, predictions)\n",
    "        self.compiled_metrics.update_state(y, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def _grad_norm(self, gradients):\n",
    "        norm = tf.norm(\n",
    "            tf.stack([\n",
    "                tf.norm(grad) for grad in gradients if grad is not None\n",
    "            ])\n",
    "        )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d537327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_cfg, model_name, build_fn, training, do_SAM, summary=False, model_cnt=0, learning_rate=0.0003):        \n",
    "    model_name_list.append(model_name)\n",
    "    \n",
    "    CALLBACKS = [\n",
    "    tf.keras.callbacks.EarlyStopping(mode='min', monitor='val_mae', verbose=True, restore_best_weights=True, patience=9), \n",
    "    tf.keras.callbacks.ReduceLROnPlateau(mode='min', monitor='val_mae', verbose=True, factor=0.3, patience=4, min_delta=0), \n",
    "    tf.keras.callbacks.ModelCheckpoint(model_name, mode='min', monitor='val_mae', verbose=True, save_best_only=True,\n",
    "                                      save_weights_only=True, \n",
    "                                      ),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=f'./logs/{model_name}', histogram_freq=1)\n",
    "            ]\n",
    "    \n",
    "    if summary:\n",
    "        build_fn(**model_cfg).summary()\n",
    "    \n",
    "    opt = AdamW(weight_decay=0.00003, learning_rate=learning_rate)\n",
    "    \n",
    "    if do_SAM:\n",
    "        model = SAMModel(build_fn, rho=0.03, params=model_cfg)\n",
    "    else:\n",
    "        model = build_fn(**model_cfg)\n",
    "    \n",
    "    model.compile(loss='mae', optimizer=opt, metrics='mae')\n",
    "    \n",
    "    if training:\n",
    "        model.fit(train_ds, epochs=200, callbacks=CALLBACKS, validation_data=val_ds)\n",
    "    \n",
    "    if do_SAM:\n",
    "        model.built=True\n",
    "    \n",
    "    model.load_weights(model_name)\n",
    "    \n",
    "    val_pred = model.predict(Xvalid, batch_size=BATCH_SIZE*2)\n",
    "    val_score = get_anomaly_score(Yvalid, val_pred, dynamic=0)\n",
    "    val_pred_df[f\"model_{model_cnt}\"] = list(np.zeros(TIMESTEP)) + list(val_score.ravel())\n",
    "    \n",
    "    preds = []\n",
    "    for Xtest, Ytest in tqdm(zip(Xtests, Ytests)):\n",
    "        test_pred = model.predict(Xtest, batch_size=BATCH_SIZE*2)\n",
    "        test_score = get_anomaly_score(Ytest, test_pred, dynamic=0)\n",
    "        preds = preds + list(np.zeros(TIMESTEP)) + list(test_score.ravel())\n",
    "\n",
    "    test_pred_df[f\"model_{model_cnt}\"] = preds\n",
    "\n",
    "    val_pred_df.to_csv('val_pred_df.csv', index=False)\n",
    "    test_pred_df.to_csv('test_pred_df.csv', index=False)\n",
    "    \n",
    "    return model_cnt + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab357141",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac34ab74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "MODEL_CNT = test_pred_df.shape[1] #0\n",
    "print(MODEL_CNT)\n",
    "BATCH_SIZE = 512\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "os.makedirs('./logs', exist_ok=True)\n",
    "model_name_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e508a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When writing new tfrecord files : set write=True\n",
    "# if write=False, It just loads existing tfrecords\n",
    "# When training new models again : set training=True\n",
    "\n",
    "training = False\n",
    "write = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab88da",
   "metadata": {},
   "source": [
    "# Timestep : 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5e25498",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEP = 19\n",
    "\n",
    "train_ds, val_ds = get_train_dataset(suffix='', test_size=0.1, shuffle=True, write=write)\n",
    "Xvalid, Yvalid, attack = get_valid_dataset()\n",
    "Xtests, Ytests = get_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3d23488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:08,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Models : 1\n"
     ]
    }
   ],
   "source": [
    "model_cfg = {\n",
    "             'num_blocks' : 3, \n",
    "             'embed' : 256,\n",
    "             'token_mixing' : 64, \n",
    "             'channel_mixing' : 128,\n",
    "             'dropout' : 0.5, \n",
    "            }\n",
    "model_name = \"{}model{}-Mixer({},{},{},{}).h5\".format(model_dir, TIMESTEP, \n",
    "                                                                model_cfg['num_blocks'], \n",
    "                                                                model_cfg['embed'], \n",
    "                                                                model_cfg['token_mixing'], \n",
    "                                                                model_cfg['channel_mixing']\n",
    "                                                               )\n",
    "MODEL_CNT = run_model(model_cfg, model_name, build_fn=build_mixer, training=training, \n",
    "                      do_SAM=True, summary=False, model_cnt=MODEL_CNT)\n",
    "print(f\"Trained Models : {MODEL_CNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54851d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:10,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Models : 2\n"
     ]
    }
   ],
   "source": [
    "model_cfg = {\n",
    "             'num_blocks' : 3, \n",
    "             'embed' : 256,\n",
    "             'token_mixing' : 128, \n",
    "             'channel_mixing' : 128,\n",
    "             'dropout' : 0.5, \n",
    "            }\n",
    "model_name = \"{}model{}-Mixer({},{},{},{}).h5\".format(model_dir, TIMESTEP, \n",
    "                                                                model_cfg['num_blocks'], \n",
    "                                                                model_cfg['embed'], \n",
    "                                                                model_cfg['token_mixing'], \n",
    "                                                                model_cfg['channel_mixing']\n",
    "                                                               )\n",
    "MODEL_CNT = run_model(model_cfg, model_name, build_fn=build_mixer, training=training, \n",
    "                      do_SAM=True, summary=False, model_cnt=MODEL_CNT)\n",
    "print(f\"Trained Models : {MODEL_CNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfc5d1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:15,  5.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Models : 3\n"
     ]
    }
   ],
   "source": [
    "model_cfg = {\n",
    "             'num_blocks' : 3, \n",
    "             'embed' : 256,\n",
    "             'token_mixing' : 256, \n",
    "             'channel_mixing' : 128,\n",
    "             'dropout' : 0.5, \n",
    "            }\n",
    "\n",
    "model_name = \"{}model{}-Mixer({},{},{},{}).h5\".format(model_dir, TIMESTEP, \n",
    "                                                                model_cfg['num_blocks'], \n",
    "                                                                model_cfg['embed'], \n",
    "                                                                model_cfg['token_mixing'], \n",
    "                                                                model_cfg['channel_mixing']\n",
    "                                                               )\n",
    "\n",
    "MODEL_CNT = run_model(model_cfg, model_name, build_fn=build_mixer, training=training, \n",
    "                      do_SAM=True, summary=False, model_cnt=MODEL_CNT)\n",
    "print(f\"Trained Models : {MODEL_CNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86b0060e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:10,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Models : 4\n"
     ]
    }
   ],
   "source": [
    "model_cfg = {\n",
    "             'num_blocks' : 3, \n",
    "             'embed' : 256,\n",
    "             'token_mixing' : 64, \n",
    "             'channel_mixing' : 512,\n",
    "             'dropout' : 0.5, \n",
    "            }\n",
    "\n",
    "model_name = \"{}model{}-Mixer({},{},{},{}).h5\".format(model_dir, TIMESTEP, \n",
    "                                                                model_cfg['num_blocks'], \n",
    "                                                                model_cfg['embed'], \n",
    "                                                                model_cfg['token_mixing'], \n",
    "                                                                model_cfg['channel_mixing']\n",
    "                                                               )\n",
    "\n",
    "MODEL_CNT = run_model(model_cfg, model_name, build_fn=build_mixer, training=training, \n",
    "                      do_SAM=True, summary=False, model_cnt=MODEL_CNT)\n",
    "print(f\"Trained Models : {MODEL_CNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4e40c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:12,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Models : 5\n"
     ]
    }
   ],
   "source": [
    "model_cfg = {\n",
    "             'num_blocks' : 3, \n",
    "             'embed' : 256,\n",
    "             'token_mixing' : 128, \n",
    "             'channel_mixing' : 512,\n",
    "             'dropout' : 0.5, \n",
    "            }\n",
    "\n",
    "model_name = \"{}model{}-Mixer({},{},{},{}).h5\".format(model_dir, TIMESTEP, \n",
    "                                                                model_cfg['num_blocks'], \n",
    "                                                                model_cfg['embed'], \n",
    "                                                                model_cfg['token_mixing'], \n",
    "                                                                model_cfg['channel_mixing']\n",
    "                                                               )\n",
    "\n",
    "MODEL_CNT = run_model(model_cfg, model_name, build_fn=build_mixer, training=training, \n",
    "                      do_SAM=True, summary=False, model_cnt=MODEL_CNT)\n",
    "\n",
    "print(f\"Trained Models : {MODEL_CNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ca93639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:17,  5.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Models : 6\n"
     ]
    }
   ],
   "source": [
    "model_cfg = {\n",
    "             'num_blocks' : 3, \n",
    "             'embed' : 256,\n",
    "             'token_mixing' : 256, \n",
    "             'channel_mixing' : 512,\n",
    "             'dropout' : 0.5, \n",
    "            }\n",
    "\n",
    "model_name = \"{}model{}-Mixer({},{},{},{}).h5\".format(model_dir, TIMESTEP, \n",
    "                                                                model_cfg['num_blocks'], \n",
    "                                                                model_cfg['embed'], \n",
    "                                                                model_cfg['token_mixing'], \n",
    "                                                                model_cfg['channel_mixing']\n",
    "                                                               )\n",
    "\n",
    "MODEL_CNT = run_model(model_cfg, model_name, build_fn=build_mixer, training=training, \n",
    "                      do_SAM=True, summary=False, model_cnt=MODEL_CNT)\n",
    "\n",
    "print(f\"Trained Models : {MODEL_CNT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e2a1e",
   "metadata": {},
   "source": [
    "# Timestep : 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e9f447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEP = 29\n",
    "\n",
    "train_ds, val_ds = get_train_dataset(suffix='', test_size=0.1, shuffle=True, write=write)\n",
    "Xvalid, Yvalid, attack = get_valid_dataset()\n",
    "Xtests, Ytests = get_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c491d304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:10,  3.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Models : 7\n"
     ]
    }
   ],
   "source": [
    "model_cfg = {\n",
    "             'num_blocks' : 3, \n",
    "             'embed' : 256,\n",
    "             'token_mixing' : 64, \n",
    "             'channel_mixing' : 128,\n",
    "             'dropout' : 0.5, \n",
    "            }\n",
    "model_name = \"{}model{}-Mixer({},{},{},{}).h5\".format(model_dir, TIMESTEP, \n",
    "                                                                model_cfg['num_blocks'], \n",
    "                                                                model_cfg['embed'], \n",
    "                                                                model_cfg['token_mixing'], \n",
    "                                                                model_cfg['channel_mixing']\n",
    "                                                               )\n",
    "MODEL_CNT = run_model(model_cfg, model_name, build_fn=build_mixer, training=training, \n",
    "                      do_SAM=True, summary=False, model_cnt=MODEL_CNT)\n",
    "print(f\"Trained Models : {MODEL_CNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c3d0631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:13,  4.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Models : 8\n"
     ]
    }
   ],
   "source": [
    "model_cfg = {\n",
    "             'num_blocks' : 3, \n",
    "             'embed' : 256,\n",
    "             'token_mixing' : 128, \n",
    "             'channel_mixing' : 128,\n",
    "             'dropout' : 0.5, \n",
    "            }\n",
    "model_name = \"{}model{}-Mixer({},{},{},{}).h5\".format(model_dir, TIMESTEP, \n",
    "                                                                model_cfg['num_blocks'], \n",
    "                                                                model_cfg['embed'], \n",
    "                                                                model_cfg['token_mixing'], \n",
    "                                                                model_cfg['channel_mixing']\n",
    "                                                               )\n",
    "MODEL_CNT = run_model(model_cfg, model_name, build_fn=build_mixer, training=training, \n",
    "                      do_SAM=True, summary=False, model_cnt=MODEL_CNT)\n",
    "print(f\"Trained Models : {MODEL_CNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d83252de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:17,  5.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Models : 9\n"
     ]
    }
   ],
   "source": [
    "model_cfg = {\n",
    "             'num_blocks' : 3, \n",
    "             'embed' : 256,\n",
    "             'token_mixing' : 256, \n",
    "             'channel_mixing' : 128,\n",
    "             'dropout' : 0.5, \n",
    "            }\n",
    "\n",
    "model_name = \"{}model{}-Mixer({},{},{},{}).h5\".format(model_dir, TIMESTEP, \n",
    "                                                                model_cfg['num_blocks'], \n",
    "                                                                model_cfg['embed'], \n",
    "                                                                model_cfg['token_mixing'], \n",
    "                                                                model_cfg['channel_mixing']\n",
    "                                                               )\n",
    "\n",
    "MODEL_CNT = run_model(model_cfg, model_name, build_fn=build_mixer, training=training, \n",
    "                      do_SAM=True, summary=False, model_cnt=MODEL_CNT)\n",
    "print(f\"Trained Models : {MODEL_CNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27231619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:13,  4.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Models : 10\n"
     ]
    }
   ],
   "source": [
    "model_cfg = {\n",
    "             'num_blocks' : 3, \n",
    "             'embed' : 256,\n",
    "             'token_mixing' : 64, \n",
    "             'channel_mixing' : 512,\n",
    "             'dropout' : 0.5, \n",
    "            }\n",
    "\n",
    "model_name = \"{}model{}-Mixer({},{},{},{}).h5\".format(model_dir, TIMESTEP, \n",
    "                                                                model_cfg['num_blocks'], \n",
    "                                                                model_cfg['embed'], \n",
    "                                                                model_cfg['token_mixing'], \n",
    "                                                                model_cfg['channel_mixing']\n",
    "                                                               )\n",
    "\n",
    "MODEL_CNT = run_model(model_cfg, model_name, build_fn=build_mixer, training=training, \n",
    "                      do_SAM=True, summary=False, model_cnt=MODEL_CNT)\n",
    "print(f\"Trained Models : {MODEL_CNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0622611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:15,  5.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Models : 11\n"
     ]
    }
   ],
   "source": [
    "model_cfg = {\n",
    "             'num_blocks' : 3, \n",
    "             'embed' : 256,\n",
    "             'token_mixing' : 128, \n",
    "             'channel_mixing' : 512,\n",
    "             'dropout' : 0.5, \n",
    "            }\n",
    "\n",
    "model_name = \"{}model{}-Mixer({},{},{},{}).h5\".format(model_dir, TIMESTEP, \n",
    "                                                                model_cfg['num_blocks'], \n",
    "                                                                model_cfg['embed'], \n",
    "                                                                model_cfg['token_mixing'], \n",
    "                                                                model_cfg['channel_mixing']\n",
    "                                                               )\n",
    "\n",
    "MODEL_CNT = run_model(model_cfg, model_name, build_fn=build_mixer, training=training, \n",
    "                      do_SAM=True, summary=False, model_cnt=MODEL_CNT)\n",
    "\n",
    "print(f\"Trained Models : {MODEL_CNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4812b459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:20,  6.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Models : 12\n"
     ]
    }
   ],
   "source": [
    "model_cfg = {\n",
    "             'num_blocks' : 3, \n",
    "             'embed' : 256,\n",
    "             'token_mixing' : 256, \n",
    "             'channel_mixing' : 512,\n",
    "             'dropout' : 0.5, \n",
    "            }\n",
    "\n",
    "model_name = \"{}model{}-Mixer({},{},{},{}).h5\".format(model_dir, TIMESTEP, \n",
    "                                                                model_cfg['num_blocks'], \n",
    "                                                                model_cfg['embed'], \n",
    "                                                                model_cfg['token_mixing'], \n",
    "                                                                model_cfg['channel_mixing']\n",
    "                                                               )\n",
    "\n",
    "MODEL_CNT = run_model(model_cfg, model_name, build_fn=build_mixer, training=False, \n",
    "                      do_SAM=True, summary=False, model_cnt=MODEL_CNT)\n",
    "\n",
    "print(f\"Trained Models : {MODEL_CNT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0276af12",
   "metadata": {},
   "source": [
    "# Timestep : 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5580becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEP = 39\n",
    "\n",
    "train_ds, val_ds = get_train_dataset(suffix='', test_size=0.1, shuffle=True, write=write)\n",
    "Xvalid, Yvalid, attack = get_valid_dataset()\n",
    "Xtests, Ytests = get_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "307eb3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:13,  4.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Models : 13\n"
     ]
    }
   ],
   "source": [
    "model_cfg = {\n",
    "             'num_blocks' : 3, \n",
    "             'embed' : 256,\n",
    "             'token_mixing' : 64, \n",
    "             'channel_mixing' : 128,\n",
    "             'dropout' : 0.5, \n",
    "            }\n",
    "model_name = \"{}model{}-Mixer({},{},{},{}).h5\".format(model_dir, TIMESTEP, \n",
    "                                                                model_cfg['num_blocks'], \n",
    "                                                                model_cfg['embed'], \n",
    "                                                                model_cfg['token_mixing'], \n",
    "                                                                model_cfg['channel_mixing']\n",
    "                                                               )\n",
    "MODEL_CNT = run_model(model_cfg, model_name, build_fn=build_mixer, training=training, \n",
    "                      do_SAM=True, summary=False, model_cnt=MODEL_CNT)\n",
    "print(f\"Trained Models : {MODEL_CNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c11bc36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:15,  5.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Models : 14\n"
     ]
    }
   ],
   "source": [
    "model_cfg = {\n",
    "             'num_blocks' : 3, \n",
    "             'embed' : 256,\n",
    "             'token_mixing' : 128, \n",
    "             'channel_mixing' : 128,\n",
    "             'dropout' : 0.5, \n",
    "            }\n",
    "model_name = \"{}model{}-Mixer({},{},{},{}).h5\".format(model_dir, TIMESTEP, \n",
    "                                                                model_cfg['num_blocks'], \n",
    "                                                                model_cfg['embed'], \n",
    "                                                                model_cfg['token_mixing'], \n",
    "                                                                model_cfg['channel_mixing']\n",
    "                                                               )\n",
    "MODEL_CNT = run_model(model_cfg, model_name, build_fn=build_mixer, training=training, \n",
    "                      do_SAM=True, summary=False, model_cnt=MODEL_CNT)\n",
    "print(f\"Trained Models : {MODEL_CNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe633949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:20,  6.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Models : 15\n"
     ]
    }
   ],
   "source": [
    "model_cfg = {\n",
    "             'num_blocks' : 3, \n",
    "             'embed' : 256,\n",
    "             'token_mixing' : 256, \n",
    "             'channel_mixing' : 128,\n",
    "             'dropout' : 0.5, \n",
    "            }\n",
    "\n",
    "model_name = \"{}model{}-Mixer({},{},{},{}).h5\".format(model_dir, TIMESTEP, \n",
    "                                                                model_cfg['num_blocks'], \n",
    "                                                                model_cfg['embed'], \n",
    "                                                                model_cfg['token_mixing'], \n",
    "                                                                model_cfg['channel_mixing']\n",
    "                                                               )\n",
    "\n",
    "MODEL_CNT = run_model(model_cfg, model_name, build_fn=build_mixer, training=training, \n",
    "                      do_SAM=True, summary=False, model_cnt=MODEL_CNT)\n",
    "print(f\"Trained Models : {MODEL_CNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "883f9e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:16,  5.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Models : 16\n"
     ]
    }
   ],
   "source": [
    "model_cfg = {\n",
    "             'num_blocks' : 3, \n",
    "             'embed' : 256,\n",
    "             'token_mixing' : 64, \n",
    "             'channel_mixing' : 512,\n",
    "             'dropout' : 0.5, \n",
    "            }\n",
    "\n",
    "model_name = \"{}model{}-Mixer({},{},{},{}).h5\".format(model_dir, TIMESTEP, \n",
    "                                                                model_cfg['num_blocks'], \n",
    "                                                                model_cfg['embed'], \n",
    "                                                                model_cfg['token_mixing'], \n",
    "                                                                model_cfg['channel_mixing']\n",
    "                                                               )\n",
    "\n",
    "MODEL_CNT = run_model(model_cfg, model_name, build_fn=build_mixer, training=training, \n",
    "                      do_SAM=True, summary=False, model_cnt=MODEL_CNT)\n",
    "print(f\"Trained Models : {MODEL_CNT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caff8eb",
   "metadata": {},
   "source": [
    "# Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "708b6724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features : 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attack</th>\n",
       "      <th>model_0</th>\n",
       "      <th>model_1</th>\n",
       "      <th>model_2</th>\n",
       "      <th>model_3</th>\n",
       "      <th>model_4</th>\n",
       "      <th>model_5</th>\n",
       "      <th>model_6</th>\n",
       "      <th>model_7</th>\n",
       "      <th>model_8</th>\n",
       "      <th>model_9</th>\n",
       "      <th>model_10</th>\n",
       "      <th>model_11</th>\n",
       "      <th>model_12</th>\n",
       "      <th>model_13</th>\n",
       "      <th>model_14</th>\n",
       "      <th>model_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   attack  model_0  model_1  model_2  model_3  model_4  model_5  model_6  \\\n",
       "0       0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1       0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2       0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3       0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4       0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   model_7  model_8  model_9  model_10  model_11  model_12  model_13  \\\n",
       "0      0.0      0.0      0.0       0.0       0.0       0.0       0.0   \n",
       "1      0.0      0.0      0.0       0.0       0.0       0.0       0.0   \n",
       "2      0.0      0.0      0.0       0.0       0.0       0.0       0.0   \n",
       "3      0.0      0.0      0.0       0.0       0.0       0.0       0.0   \n",
       "4      0.0      0.0      0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   model_14  model_15  \n",
       "0       0.0       0.0  \n",
       "1       0.0       0.0  \n",
       "2       0.0       0.0  \n",
       "3       0.0       0.0  \n",
       "4       0.0       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_0</th>\n",
       "      <th>model_1</th>\n",
       "      <th>model_2</th>\n",
       "      <th>model_3</th>\n",
       "      <th>model_4</th>\n",
       "      <th>model_5</th>\n",
       "      <th>model_6</th>\n",
       "      <th>model_7</th>\n",
       "      <th>model_8</th>\n",
       "      <th>model_9</th>\n",
       "      <th>model_10</th>\n",
       "      <th>model_11</th>\n",
       "      <th>model_12</th>\n",
       "      <th>model_13</th>\n",
       "      <th>model_14</th>\n",
       "      <th>model_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_0  model_1  model_2  model_3  model_4  model_5  model_6  model_7  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   model_8  model_9  model_10  model_11  model_12  model_13  model_14  \\\n",
       "0      0.0      0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1      0.0      0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "2      0.0      0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3      0.0      0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4      0.0      0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   model_15  \n",
       "0       0.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_pred_df = pd.read_csv('val_pred_df.csv')\n",
    "test_pred_df = pd.read_csv('test_pred_df.csv')\n",
    "print(f\"Features : {test_pred_df.shape[1]}\")\n",
    "\n",
    "ATTACK = val_pred_df['attack']\n",
    "drop_cols = []\n",
    "val_pred_df.drop(columns=drop_cols, inplace=True)\n",
    "test_pred_df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "display(val_pred_df.head())\n",
    "display(test_pred_df.head())\n",
    "verbose=False\n",
    "\n",
    "if verbose:\n",
    "    optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "else:\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e0c70f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mas = [3, 17, 17, 3, 30, 28, 7, 29, 6, 10, 15, 20, 27, 11, 27, 30]\n",
    "thresholds = [0.0038036438369408645, 0.0016718090333392643, 0.009060589258278696, 0.001876418496633655, \n",
    "0.0010295969251902545, 0.0016506555965589535, 0.006195710137649554, 0.003144810692142613, 0.00435580584791175, \n",
    "0.0025270679496686003, 0.0030780698595510966, 0.0032127646102156227, 0.0021219862581206626,\n",
    " 0.0034109746173487123, 0.003273907662555046, 0.0013015271849548958]\n",
    "weights = [2.76714374e-04, 8.40115903e-03, 1.13912089e-02, 2.24516584e-04,\n",
    " 5.98973424e-04, 3.64605453e-03, 2.26548160e-05, 6.26152324e-03,\n",
    " 4.77642688e-01, 3.64163688e-01, 9.42076776e-04, 3.15108554e-05,\n",
    " 2.15617205e-03, 3.88060831e-05, 1.24078144e-01, 1.24109337e-04]\n",
    "\n",
    "val_result = []\n",
    "test_result = []\n",
    "\n",
    "for ma, threshold, col in zip(mas, thresholds, test_pred_df.columns):\n",
    "    val_result.append(get_predictions(ma_smoothing(val_pred_df[col], ma), threshold))\n",
    "    test_result.append(get_predictions(ma_smoothing(test_pred_df[col], ma), threshold))\n",
    "\n",
    "val_result = np.stack(val_result, axis=1)\n",
    "test_result = np.stack(test_result, axis=1)\n",
    "\n",
    "val_final = np.round((val_result * weights).sum(axis=1)).astype('int')\n",
    "test_final = np.round((test_result * weights).sum(axis=1)).astype('int')\n",
    "\n",
    "submission['attack'] = test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4adfe812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.552 (TaP: 0.540, TaR: 0.565)\n",
      "Detected anomalies: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    267974\n",
       "1      6826\n",
       "Name: attack, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score(ATTACK, val_final, verbose=True)\n",
    "submission['attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "58c9e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(submit_dir + 'FinalSubmission1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
